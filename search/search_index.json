{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Magnus Extensions","text":"<p>This repository provides all the extensions to magnus core package.</p> <p>Magnus provides 5 essential services:</p> <ul> <li>Executor: A way to define and execute/transpile dag definition.</li> <li>Catalog: An artifact store used to log and store data files generated during a pipeline execution.</li> <li>Secrets Handler: A framework to handle secrets from different providers.</li> <li>Logging: A comprehensive and automatic logging framework to capture essential information of a pipeline execution.</li> <li>Experiment Tracking: A framework to interact with different experiment tracking tools.</li> </ul>"},{"location":"#a_schematic_to_show_what_it_does","title":"A schematic to show what it does","text":""},{"location":"#available_extensions_to_the_services","title":"Available extensions to the services:","text":"Service Description Availability Executors Local Run the pipeline on local machine (default) Part of Magnus core Local Containers Run the pipeline on local containers Part of Magnus core Kubeflow Transpile using Kubeflow pipelines Part of extensions Argo Workflows Transpile to Argo Workflow specification Part of extensions K8s Job Executes notebooks/functions in Kubernetes Part of extensions Catalog Do Nothing Provides no cataloging functionality Part of Magnus core File System Uses local file system (default) Part of Magnus core S3 Uses S3 as a catalog Part of extensions K8s PVB Uses Persistent volumes in K8s as a catalog Part of extensions Secrets Do Nothing Provides no secrets handler (default) Part of Magnus core Dot Env Uses a file as secrets Part of Magnus core Environment Variables Gets secrets from Environmental variables Part of Magnus core Logging Buffered Uses the run time buffer as logger (default) Part of Magnus core File System Uses a file system as run log store Part of Magnus core Chunked File System Uses a file system but thread safe Part of magnus core S3 Uses S3 to store logs Part of extensions Chunked S3 Uses S3 to store logs but thread safe Part of extensions K8s PVC Uses persistent volumes of K8s as run log store Part of extensions K8s PVC Chunked Uses persistent volumes of K8s as run log store but thread safe Part of extensions Experiment Tracking Do Nothing Provides no experiment tracking (default) Part of Magnus core ML Flow Provides MLFlow experiment tracking Part of extensions"},{"location":"installation/","title":"Installation","text":"<p>The minimum python version that magnus supports is 3.8</p>"},{"location":"installation/#pip","title":"pip","text":"<p>magnus-extensions is a python package and should be installed as any other.</p> <pre><code>pip install magnus-extensions\n</code></pre> <p>We recommend that you install magnus in a virtual environment specific to the project and also poetry for your application development.</p> <p>The command to install in a poetry managed virtual environment</p> <pre><code>poetry add magnus-extensions\n</code></pre> <p>Individual extensions require additional packages that are detailed as part of the extension.</p>"},{"location":"catalog/s3/","title":"S3","text":"<p>S3 is widely used a data storage and magnus can use S3 as a catalogue storage.</p>"},{"location":"catalog/s3/#additional_dependencies","title":"Additional dependencies","text":"<p>Magnus extensions needs AWS capabilities via boto3 to use S3. You can install it via</p> <p><code>pip install magnus_extensions[aws]</code></p> <p>or via:</p> <p><code>poetry add magnus_extensions[aws]</code></p>"},{"location":"catalog/s3/#configuration","title":"Configuration","text":"<p>The full configuration to use S3 as data catalog:</p> <pre><code>catalog:\ntype: s3\nconfig:\naws_profile: str # defaults to ''\nuse_credentials: bool # defaults to False\nregion: str # defaults to eu-west-1\naws_credentials_file: str # defaults to str(Path.home() / '.aws' / 'credentials')\naws_access_key_name: str # defaults to  'AWS_ACCESS_KEY_ID'\naws_secret_access_key_name: str # defaults to 'AWS_SECRET_ACCESS_KEY'\naws_session_key_name: str # defaults to 'AWS_SESSION_TOKEN'\nrole_arn: str # defaults to ''\nsession_duration_in_seconds: int # defaults to 900\ncompute_data_folder : str # defaults to data/\ns3_bucket: str # Should be PROVIDED\nprefix: str # defaults to str\n</code></pre> <p>The <code>data</code> folder that is used for your work.</p> <p>Logically cataloging works as follows:</p> <ul> <li>get files from the catalog before the execution to a specific compute data folder</li> <li>execute the command</li> <li>put the files from the compute data folder to the catalog.</li> </ul> <p>You can over-ride the compute data folder, defined globally, for individual steps by providing it in the step configuration.</p> <p>For example:</p> <pre><code>catalog:\n...\n\ndag:\nsteps:\nstep name:\n...\ncatalog:\ncompute_data_folder: # optional and only apples to this step\nget:\n- list\nput:\n- list\n\n...\n</code></pre> <ul> <li> </li> </ul> <p>The s3 bucket to use as a catalog</p> <ul> <li> </li> </ul> <p>The prefix to the path where the cataloging is done.</p> <p>For example: if the prefix is <code>catalog</code>, then the catalog per run would be stored at:</p> <p><code>&lt;s3_bucket&gt;/catalog/&lt;run_id&gt;/</code>.</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> </ul>"},{"location":"catalog/s3/#compute_data_folder","title":"compute_data_folder:","text":""},{"location":"catalog/s3/#s3_bucket","title":"s3_bucket:","text":""},{"location":"catalog/s3/#prefix","title":"prefix:","text":""},{"location":"catalog/s3/#aws_profile","title":"aws_profile:","text":"<p>Defaults to '' or the <code>default</code> profile.</p>"},{"location":"catalog/s3/#use_credentials","title":"use_credentials:","text":"<p>Defaults to False. It is always safer to use RBAC instead of credentials.</p>"},{"location":"catalog/s3/#region","title":"region:","text":"<p>Defaults to eu-west-1. The AWS region you want a boto3 session to be instantiated.</p>"},{"location":"catalog/s3/#aws_credentials_file","title":"aws_credentials_file:","text":"<p>Defaults to str(Path.home() / '.aws' / 'credentials').   The file where AWS credentials are typically stored. This file is used in both use_credentials and by internally by   boto3 while looking for profiles.</p>"},{"location":"catalog/s3/#aws_access_key_name","title":"aws_access_key_name:","text":"<p>Defaults to  'AWS_ACCESS_KEY_ID'.   The environmental variable name that is to be used as aws access key, if you are using credentials.</p>"},{"location":"catalog/s3/#aws_secret_access_key_name","title":"aws_secret_access_key_name:","text":"<p>Defaults to 'AWS_SECRET_ACCESS_KEY'.   The environmental variable name that is used as AWS Secret access key, if you are using credentials.</p>"},{"location":"catalog/s3/#aws_session_key_name","title":"aws_session_key_name:","text":"<p>Defaults to 'AWS_SESSION_TOKEN'   The environmental variable name that is used for AWS session token, if you are using credentials.</p>"},{"location":"catalog/s3/#role_arn","title":"role_arn:","text":"<p>Defaults to ''.   The role to assume if you are using sessions.</p>"},{"location":"catalog/s3/#session_duration_in_seconds","title":"session_duration_in_seconds:","text":"<p>Defaults to 900   The duration of the AWS session.</p>"},{"location":"executor/argo/","title":"Argo workflows","text":"<p>Argo workflows is a powerful workflow specification engine for K8's. Magnus can transpile the pipeline definition to a Argo workflow specification.</p>"},{"location":"executor/argo/#features_not_implemented","title":"Features not implemented","text":"<ul> <li>We have not yet implemented parallel, dag or map state in this extension.</li> <li>on_failure has not be implemented yet.</li> </ul>"},{"location":"executor/argo/#additional_dependencies","title":"Additional dependencies","text":"<p>Since argo is a cloud based orchestration tool, other services which are not accessible by cloud would not work.</p>"},{"location":"executor/argo/#configuration","title":"Configuration","text":"<p>The full configuration to use Argo extension:</p> <pre><code>executor:\ntype: \"argo\"\nconfig:\ndocker_image: str\noutput_file: str\ncpu_limit: str\nmemory_limit: str\ncpu_request: str\nmemory_request: str\nenable_caching: bool\nimage_pull_policy: str\nsecrets_from_k8s: dict\n</code></pre> <p>Individual steps of the dag can over-ride the configuration by providing a <code>mode_config</code> section.</p> <pre><code>dag:\nsteps:\nstep:\n...\nmode_config:\nargo:\ndocker_image: #\u00a0Overrides the default docker_image\nsecrets_from_k8s: #\u00a0Overrides the default secrets_from_k8's\n...\ncpu_limit: # Overrides the default cpu_limit\ncpu_request: #\u00a0Overrides the default cpu_request\nmemory_limit: #\u00a0Overrides the default memory_limit\nmemory_request: #\u00a0Overrides the default memory_request\nimage_pull_policy: #\u00a0Overrides the default image pull policy\n...\n</code></pre> <p>The docker image to use to run the dag/step.</p> <p>The name of the output file to write the generated pipeline definition via Kubeflow. Defaults to <code>pipeline.yaml</code>.</p> <p>The default cpu limit from K8's. Defaults to 250m.</p> <p>The default memory limit from K8's. Defaults to 1G.</p> <p>The default cpu to request from K8's. If not provided, it is the same as default_cpu_limit.</p> <p>The default memory request from K8's. if not provided, it is the same as default_memory_limit.</p> <p>Controls the caching behavior of Kubeflow, defaults to False.</p> <p>Set to \"Always\", the available options are: \"IfNotPresent\", \"Always\", \"Never\".</p> <p>Warning</p> <p>Use \"IfNotPresent\" cautiously, as the check happens on the tag of the docker image and an improper versioning strategy might result in wrong docker images being used.</p> <p>Use secrets stored in underlying K8's while running the containers. The format is <code>EnvVar=SecretName:Key</code> where</p> <pre><code>- EnvVar is the name of the Environment variable the secret should be in the container.\n- SecretName: The name of the secret in K8's.\n- Key: The key in the secret that should be exposed in the container.\n</code></pre>"},{"location":"executor/argo/#docker_image","title":"docker_image:","text":""},{"location":"executor/argo/#output_file","title":"output_file:","text":""},{"location":"executor/argo/#cpu_limit","title":"cpu_limit:","text":""},{"location":"executor/argo/#memory_limit","title":"memory_limit:","text":""},{"location":"executor/argo/#cpu_request","title":"cpu_request:","text":""},{"location":"executor/argo/#memory_request","title":"memory_request:","text":""},{"location":"executor/argo/#enable_caching","title":"enable_caching:","text":""},{"location":"executor/argo/#image_pull_policy","title":"image_pull_policy:","text":""},{"location":"executor/argo/#secrets_from_k8s","title":"secrets_from_k8s:","text":""},{"location":"executor/argo/#parameters","title":"Parameters","text":"<p>All the parameters that are defined via the <code>--parameters</code> option are available via the Argo CLI or WebUI of kubeflow to configure during run time.</p>"},{"location":"executor/argo/#design_guidelines","title":"Design Guidelines","text":"<ul> <li>As the container is generated before the argo workflow is generated, it is a good idea to parameterize the docker image id as part of the definition.</li> </ul> <pre><code>mode:\ntype: \"argo\"\nconfig:\ndocker_image: ${docker_image}\n</code></pre> <p>The variable <code>docker_image</code> can then be provided during the run time of magnus.</p> <pre><code>export MAGNUS_VAR_docker_image=my_cool_image\nmagnus execute --file getting-started.yaml --config kubeflow.yaml\n</code></pre> <ul> <li>Generate and deploy the argo pipelines as part of the CI.</li> </ul>"},{"location":"executor/k8s-job/","title":"Jobs on Kubernetes","text":"<p>Kubernetes is a powerful cloud agnostic platform and this extension provides a way to run batch jobs on Kubernetes. Note that this extension is only for jobs and not for any pipelines. Please refer to argo or Kubeflow to run pipelines on Kubernetes.</p>"},{"location":"executor/k8s-job/#additional_dependencies","title":"Additional dependencies","text":"<p>Magnus extensions needs additional packages to use this extension. Please install magnus-extensions via:</p> <p><code>pip install \"magnus_extensions[k8s]\"</code></p> <p>or</p> <p><code>poetry add \"magnus_extensions[k8s]\"</code></p> <p>Since kubernetes is a cloud based job scheduler, other services which are not accessible by cloud would not work.</p>"},{"location":"executor/k8s-job/#configuration","title":"Configuration:","text":"<pre><code>executor:\ntype: \"kfp\"\nconfig:\nconfig_path: str # Required\ndocker_image: str # Required\nnamespace: str # Defaults to \"default\"\ncpu_limit: str # Defaults to \"250m\"\nmemory_limit: str # Defaults to \"1G\"\ngpu_limit: int # Defaults to 0\ngpu_vendor: str # Defaults to \"nvidia.com/gpu\"\ncpu_request: str # Defaults to cpu_limit\nmemory_request: str # Defaults to memory_limit\nactive_deadline_seconds: int # Defaults to 2 hours\nttl_seconds_after_finished: int   # \u00a0Defaults to 1 minute\nimage_pull_policy: str # Defaults to  \"Always\"\nsecrets_from_k8s: dict # EnvVar=SecretName:Key\npersistent_volumes: dict # volume-name:mount_path\nlabels: Dict[str, str]\n</code></pre> <p>The location of the kubeconfig file to submit jobs.</p> <p>The docker image to use to run the job. The docker image should be accessible from the Kubernetes cluster.</p> <p>The namespace of the Kubernetes cluster to submit the jobs to. It defaults to \"default\".</p> <p>The default CPU limit for Kubernetes job. Defaults to \"250m\". Please refer to this documentation to understand more</p> <p>The default memory limit for Kubernetes job. Defaults to 1G Please refer to this documentation to understand more</p> <p>The default GPU limit for Kubernetes job. Defaults to 0. Please refer to this documentation to understand more</p> <p>The GPU type to use for Kubernetes job. The cluster should support the GPU type for this to work. Defaults to nvidia.com/gpu. [Please refer to this documentation to understand more.]https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/</p> <p>The default CPU request for Kubernetes job. Defaults to cpu_limit. Please refer to this documentation to understand more</p> <p>The default memory request for Kubernetes job. Defaults to memory_limit Please refer to this documentation to understand more</p> <p>The maximum amount of time that the job can run on the kubernetes cluster. Defaults to 2 hours. Please use this value appropriately for your job.</p> <p>Please refer to this documentation to understand more.</p> <p>The amount of time that the job/pod should be active after completing the job. Defaults to 1 minute. Please increase this time (in seconds) if you want to look into more debugging information.</p> <p>Set to \"Always\", the available options are: \"IfNotPresent\", \"Always\", \"Never\".</p> <p>Warning</p> <p>Use \"IfNotPresent\" cautiously, as the check happens on the tag of the docker image and an improper versioning strategy might result in wrong docker images being used.</p> <p>Use secrets stored in underlying K8's while running the containers. The format is <code>EnvVar=SecretName:Key</code> where</p> <pre><code>- EnvVar is the name of the Environment variable the secret should be in the container.\n- SecretName: The name of the secret in K8's.\n- Key: The key in the secret that should be exposed in the container.\n</code></pre> <p>Volumes to mount from the underlying cluster onto the container during the execution of the job.</p> <p>The format is <code>name-of-the-volume:mountpoint</code>.</p> <p>Any labels that you wish to apply to the job.</p>"},{"location":"executor/k8s-job/#config_path","title":"config_path","text":""},{"location":"executor/k8s-job/#docker_image","title":"docker_image","text":""},{"location":"executor/k8s-job/#namespace","title":"namespace","text":""},{"location":"executor/k8s-job/#cpu_limit","title":"cpu_limit","text":""},{"location":"executor/k8s-job/#memory_limit","title":"memory_limit","text":""},{"location":"executor/k8s-job/#gpu_limit","title":"gpu_limit","text":""},{"location":"executor/k8s-job/#gpu_vendor","title":"gpu_vendor","text":""},{"location":"executor/k8s-job/#cpu_request","title":"cpu_request","text":""},{"location":"executor/k8s-job/#memory_request","title":"memory_request","text":""},{"location":"executor/k8s-job/#active_deadline_seconds","title":"active_deadline_seconds","text":""},{"location":"executor/k8s-job/#ttl_seconds_after_finished","title":"ttl_seconds_after_finished","text":""},{"location":"executor/k8s-job/#image_pull_policy","title":"image_pull_policy:","text":""},{"location":"executor/k8s-job/#secrets_from_k8s","title":"secrets_from_k8s:","text":""},{"location":"executor/k8s-job/#persistent_volumes","title":"persistent_volumes","text":""},{"location":"executor/k8s-job/#labels","title":"labels","text":""},{"location":"executor/kfp/","title":"Kubeflow","text":"<p>Kubeflow pipelines is a popular tool to orchestrate ML jobs. Magnus can transpile the pipeline definition to a Kubeflow pipeline.</p>"},{"location":"executor/kfp/#features_not_implemented","title":"Features not implemented","text":"<ul> <li>We have not yet implemented parallel, dag or map state in this extension.</li> <li>Kubeflow does not have an easy way to have \"on_failure\", this feature is not yet implemented.</li> <li>We do not use any <code>volumes</code> or <code>pvolumes</code> resources defined by kubeflow as part of this extension.</li> </ul> <p>In general, we might prefer to transpile to Argo directly instead of going via Kubeflow in the future.</p>"},{"location":"executor/kfp/#additional_dependencies","title":"Additional dependencies","text":"<p>Magnus extensions needs additional packages to use this extension. Please install magnus-extensions via:</p> <p><code>pip install magnus_extensions[kubeflow]</code></p> <p>or</p> <p><code>poetry add magnus_extensions[kubeflow]</code></p> <p>Since kubeflow is a cloud based orchestration tool, other services which are not accessible by cloud would not work.</p>"},{"location":"executor/kfp/#configuration","title":"Configuration","text":"<p>The full configuration to use Kubeflow extension:</p> <pre><code>executor:\ntype: \"kfp\"\nconfig:\ndocker_image: str\noutput_file: str\ncpu_limit: str\nmemory_limit: str\ncpu_request: str\nmemory_request: str\ngpu_limit: int\nenable_caching: bool\nimage_pull_policy: str\nsecrets_from_k8s: dict\n</code></pre> <p>Individual steps of the dag can over-ride the configuration by providing a <code>mode_config</code> section.</p> <pre><code>dag:\nsteps:\nstep:\n...\nmode_config:\nkfp:\ndocker_image: #\u00a0Overrides the default docker_image\nsecrets_from_k8s: #\u00a0Overrides the default secrets_from_k8's\n...\ncpu_limit: # Overrides the default cpu_limit\ncpu_request: #\u00a0Overrides the default cpu_request\nmemory_limit: #\u00a0Overrides the default memory_limit\nmemory_request: #\u00a0Overrides the default memory_request\nimage_pull_policy: #\u00a0Overrides the default image pull policy\n...\n</code></pre> <p>The docker image to use to run the dag/step.</p> <p>The name of the output file to write the generated pipeline definition via Kubeflow. Defaults to <code>pipeline.yaml</code>.</p> <p>The default cpu limit from K8's. Defaults to 250m.</p> <p>The default memory limit from K8's. Defaults to 1G.</p> <p>The default cpu to request from K8's. If not provided, it is the same as default_cpu_limit.</p> <p>The default memory request from K8's. if not provided, it is the same as default_memory_limit.</p> <p>The gpu's to request from K8's. Defaults to 0.</p> <p>Controls the caching behavior of Kubeflow, defaults to False.</p> <p>Set to \"Always\", the available options are: \"IfNotPresent\", \"Always\", \"Never\".</p> <p>Warning</p> <p>Use \"IfNotPresent\" cautiously, as the check happens on the tag of the docker image and an improper versioning strategy might result in wrong docker images being used.</p> <p>Use secrets stored in underlying K8's while running the containers. The format is <code>EnvVar=SecretName:Key</code> where</p> <pre><code>- EnvVar is the name of the Environment variable the secret should be in the container.\n- SecretName: The name of the secret in K8's.\n- Key: The key in the secret that should be exposed in the container.\n</code></pre>"},{"location":"executor/kfp/#docker_image","title":"docker_image:","text":""},{"location":"executor/kfp/#output_file","title":"output_file:","text":""},{"location":"executor/kfp/#cpu_limit","title":"cpu_limit:","text":""},{"location":"executor/kfp/#memory_limit","title":"memory_limit:","text":""},{"location":"executor/kfp/#cpu_request","title":"cpu_request:","text":""},{"location":"executor/kfp/#memory_request","title":"memory_request:","text":""},{"location":"executor/kfp/#gpu_limit","title":"gpu_limit:","text":""},{"location":"executor/kfp/#enable_caching","title":"enable_caching:","text":""},{"location":"executor/kfp/#image_pull_policy","title":"image_pull_policy:","text":""},{"location":"executor/kfp/#secrets_from_k8s","title":"secrets_from_k8s:","text":""},{"location":"executor/kfp/#parameters","title":"Parameters","text":"<p>All the parameters that are defined via the <code>--parameters</code> option are available via the WebUI of kubeflow to configure during run time.</p>"},{"location":"executor/kfp/#design_guidelines","title":"Design Guidelines","text":"<ul> <li>As the container is generated before the kubeflow pipeline is generated, it is a good idea to parameterize the docker image id as part of the definition.</li> </ul> <pre><code>mode:\ntype: \"kfp\"\nconfig:\ndocker_image: ${docker_image}\n</code></pre> <p>The variable <code>docker_image</code> can then be provided during the run time of magnus.</p> <pre><code>export MAGNUS_VAR_docker_image=my_cool_image\nmagnus execute --file getting-started.yaml --config kubeflow.yaml\n</code></pre> <ul> <li>Generate and deploy the kubeflow pipelines as part of the CI.</li> </ul>"},{"location":"experiment_tracking/mlflow/","title":"MLFlow","text":"<p>MLFlow is a popular experiment tracking tool. We provide an integration of magnus with MLFlow.</p>"},{"location":"experiment_tracking/mlflow/#features_not_yet_implemented","title":"Features not yet implemented","text":"<p>Currently the only way to pass in the credentials for the MLFlow is via environment variables. It should be possible to provide the credentials as magnus secrets and use it in the future.</p>"},{"location":"experiment_tracking/mlflow/#additional_dependencies","title":"Additional dependencies","text":"<p>Magnus extensions needs mlflow capabilities for this. You can install it via</p> <p><code>pip install \"magnus_extensions[mlflow]\"</code></p> <p>or via:</p> <p><code>poetry add \"magnus_extensions[mlflow]\"</code></p>"},{"location":"experiment_tracking/mlflow/#configuration","title":"Configuration","text":"<p>The full configuration to use mlflow as experiment tracker:</p> <pre><code>experiment_tracker:\n  type: mlflow\n  config:\n    server_url: str\n    autolog: False\n</code></pre> <p>Warning</p> <p>Auto log features might need additional dependencies that we do not install. Please install the required.</p>"},{"location":"experiment_tracking/mlflow/#server_url","title":"server_url:","text":"<p>This is a required parameter for the experiment tracker. This should be the URL of MLFlow server.</p>"},{"location":"experiment_tracking/mlflow/#autolog","title":"autolog:","text":"<p>To enable auto logging features of mlflow.</p>"},{"location":"experiment_tracking/mlflow/#how_does_it_work","title":"How does it work?","text":"<p>Detailed explanation of how experiment tracking behaves is documented here.</p> <p>Any parameters that is tracked with magnus via <code>track_this</code> would be logged into the run log store of magnus and also sent to <code>set_metric</code> of mlflow client. Since the <code>set_metric</code> expects only numeric values in tracking, the tracking parameter is sent to <code>log_param</code> of mlflow client if it is identified to be non numeric.</p>"},{"location":"experiment_tracking/mlflow/#client_context","title":"Client context","text":"<p>You can also get the client context of mlflow client by calling <code>get_experiment_tracker_context</code> method of magnus.</p> <p>For example:</p> <pre><code>from magnus import get_experiment_tracker_context\n\n\nwith context as get_experiment_tracker_context():\n    # Do anything with mlflow client\n</code></pre>"},{"location":"run_log_store/chunked-s3/","title":"Chunked S3","text":"<p>S3 is widely used a data storage and magnus can use S3 as a run log store.</p> <p>The traditional S3 log store is unreliable and suffers from race conditions if there are parallel tasks during the execution. Chunked S3 is an enhancement over that which splits the Run log into multiple thread safe components. So instead of one single <code>.json</code> file in S3, chunked-s3 would have multiple <code>.json</code> files.</p>"},{"location":"run_log_store/chunked-s3/#additional_dependencies","title":"Additional dependencies","text":"<p>Magnus extensions needs AWS capabilities via boto3 to use S3. You can install it via</p> <p><code>pip install magnus_extensions[aws]</code></p> <p>or via:</p> <p><code>poetry add magnus_extensions[aws]</code></p>"},{"location":"run_log_store/chunked-s3/#configuration","title":"Configuration","text":"<p>The full configuration to use Chunked S3 as run log store:</p> <pre><code>run_log_store:\ntype: chunked-s3\nconfig:\naws_profile: str # defaults to ''\nuse_credentials: bool # defaults to False\nregion: str # defaults to eu-west-1\naws_credentials_file: str # defaults to str(Path.home() / '.aws' / 'credentials')\naws_access_key_name: str # defaults to  'AWS_ACCESS_KEY_ID'\naws_secret_access_key_name: str # defaults to 'AWS_SECRET_ACCESS_KEY'\naws_session_key_name: str # defaults to 'AWS_SESSION_TOKEN'\nrole_arn: str # defaults to ''\nsession_duration_in_seconds: int # defaults to 900\ns3_bucket: str # Should be PROVIDED\nprefix: str # defaults to str\n</code></pre> <p>The s3 bucket to use as a catalog</p> <p>The prefix to the path where the run logs are stored.</p> <p>For example: if the prefix is <code>datastore</code>, then the run log per run would be stored at:</p> <p><code>&lt;s3_bucket&gt;/datastore/&lt;run_id&gt;.json</code>.</p>"},{"location":"run_log_store/chunked-s3/#s3_bucket","title":"s3_bucket:","text":""},{"location":"run_log_store/chunked-s3/#prefix","title":"prefix:","text":""},{"location":"run_log_store/chunked-s3/#aws_profile","title":"aws_profile:","text":"<p>Defaults to '' or the <code>default</code> profile.</p>"},{"location":"run_log_store/chunked-s3/#use_credentials","title":"use_credentials:","text":"<p>Defaults to False. It is always safer to use RBAC instead of credentials.</p>"},{"location":"run_log_store/chunked-s3/#region","title":"region:","text":"<p>Defaults to eu-west-1. The AWS region you want a boto3 session to be instantiated.</p>"},{"location":"run_log_store/chunked-s3/#aws_credentials_file","title":"aws_credentials_file:","text":"<p>Defaults to str(Path.home() / '.aws' / 'credentials').   The file where AWS credentials are typically stored. This file is used in both use_credentials and by internally by   boto3 while looking for profiles.</p>"},{"location":"run_log_store/chunked-s3/#aws_access_key_name","title":"aws_access_key_name:","text":"<p>Defaults to  'AWS_ACCESS_KEY_ID'.   The environmental variable name that is to be used as aws access key, if you are using credentials.</p>"},{"location":"run_log_store/chunked-s3/#aws_secret_access_key_name","title":"aws_secret_access_key_name:","text":"<p>Defaults to 'AWS_SECRET_ACCESS_KEY'.   The environmental variable name that is used as AWS Secret access key, if you are using credentials.</p>"},{"location":"run_log_store/chunked-s3/#aws_session_key_name","title":"aws_session_key_name:","text":"<p>Defaults to 'AWS_SESSION_TOKEN'   The environmental variable name that is used for AWS session token, if you are using credentials.</p>"},{"location":"run_log_store/chunked-s3/#role_arn","title":"role_arn:","text":"<p>Defaults to ''.   The role to assume if you are using sessions.</p>"},{"location":"run_log_store/chunked-s3/#session_duration_in_seconds","title":"session_duration_in_seconds:","text":"<p>Defaults to 900   The duration of the AWS session.</p>"},{"location":"run_log_store/database/","title":"DB Run log store provider","text":"<p>This package is an extension to magnus.</p>"},{"location":"run_log_store/database/#provides","title":"Provides","text":"<p>Provides capability to have a database as a run log store.</p> <p>This run log store is concurrent safe.</p>"},{"location":"run_log_store/database/#installation_instructions","title":"Installation instructions","text":"<p><code>pip install magnus_extension_datastore_db</code></p>"},{"location":"run_log_store/database/#set_up_required_to_use_the_extension","title":"Set up required to use the extension","text":"<p>A database schema and a role with read/write privileges.</p> <p>The DB model used by this extension is:</p> <pre><code>class DBLog(Base):\n\"\"\"\n    Base table for storing run logs in database.\n\n    In this model, we fragment the run log into logical units that are concurrent safe.\n    \"\"\"\n    __tablename__ = 'db_log'\n    pk = Column(Integer, Sequence('id_seq'), primary_key=True)\n    run_id = Column(Text)\n    attribute_key = Column(Text)  # run_log, step_internal_name, parameter_key etc\n    attribute_type = Column(Text)  # RunLog, Step, Branch, Parameter\n    attribute_value = Column(Text)  # The JSON string\n    created_at = Column(DateTime, default=datetime.datetime.utcnow)\n</code></pre> <p>Please note that <code>created_at</code> is important for ordering of the steps and events and should be always increasing for new instances (records).</p> <p>You can either create this schema using your own mechanisms or can use the handy script provided as part of this package.</p> <pre><code>from magnus_extension_datastore_db import db\ndb.create_tables(&lt;connection_string&gt;)\n</code></pre>"},{"location":"run_log_store/database/#config_parameters","title":"Config parameters","text":"<p>The full configuration of this run log store is:</p> <pre><code>run_log:\ntype: db\nconfig:\nconnection_string: The connection string to use in SQLAlchemy. Secret placeholders are fine.\n</code></pre>"},{"location":"run_log_store/database/#connection_string","title":"connection_string:","text":"<p>Please provide the connection string of the database using this variable.</p> <p>You can use placeholders for sensitive details and provide it by the secrets manager. Internally, we use python template strings to create a template and safe substitute with secrets key value pairs.</p> <p>For example, a connection string <code>'postgresql://scott:${password}@localhost:5432/mydatabase'</code> and secrets having a key value pair of <code>password=tiger</code> would result in a connection string of <code>'postgresql://scott:tiger@localhost:5432/mydatabase'</code></p>"},{"location":"run_log_store/s3/","title":"S3","text":"<p>S3 is widely used a data storage and magnus can use S3 as a run log store.</p>"},{"location":"run_log_store/s3/#features_not_yet_implemented","title":"Features not yet implemented","text":"<p>Using a single file as a run log store for an execution with parallel branches can cause race conditions. One potential solution would be to break the run log store into multiple thread safe components which is yet to be implemented.</p>"},{"location":"run_log_store/s3/#additional_dependencies","title":"Additional dependencies","text":"<p>Magnus extensions needs AWS capabilities via boto3 to use S3. You can install it via</p> <p><code>pip install magnus_extensions[aws]</code></p> <p>or via:</p> <p><code>poetry add magnus_extensions[aws]</code></p>"},{"location":"run_log_store/s3/#configuration","title":"Configuration","text":"<p>The full configuration to use S3 as run log store:</p> <pre><code>run_log_store:\ntype: s3\nconfig:\naws_profile: str # defaults to ''\nuse_credentials: bool # defaults to False\nregion: str # defaults to eu-west-1\naws_credentials_file: str # defaults to str(Path.home() / '.aws' / 'credentials')\naws_access_key_name: str # defaults to  'AWS_ACCESS_KEY_ID'\naws_secret_access_key_name: str # defaults to 'AWS_SECRET_ACCESS_KEY'\naws_session_key_name: str # defaults to 'AWS_SESSION_TOKEN'\nrole_arn: str # defaults to ''\nsession_duration_in_seconds: int # defaults to 900\ns3_bucket: str # Should be PROVIDED\nprefix: str # defaults to str\n</code></pre> <p>The s3 bucket to use as a catalog</p> <p>The prefix to the path where the run logs are stored.</p> <p>For example: if the prefix is <code>datastore</code>, then the run log per run would be stored at:</p> <p><code>&lt;s3_bucket&gt;/datastore/&lt;run_id&gt;.json</code>.</p>"},{"location":"run_log_store/s3/#s3_bucket","title":"s3_bucket:","text":""},{"location":"run_log_store/s3/#prefix","title":"prefix:","text":""},{"location":"run_log_store/s3/#aws_profile","title":"aws_profile:","text":"<p>Defaults to '' or the <code>default</code> profile.</p>"},{"location":"run_log_store/s3/#use_credentials","title":"use_credentials:","text":"<p>Defaults to False. It is always safer to use RBAC instead of credentials.</p>"},{"location":"run_log_store/s3/#region","title":"region:","text":"<p>Defaults to eu-west-1. The AWS region you want a boto3 session to be instantiated.</p>"},{"location":"run_log_store/s3/#aws_credentials_file","title":"aws_credentials_file:","text":"<p>Defaults to str(Path.home() / '.aws' / 'credentials').   The file where AWS credentials are typically stored. This file is used in both use_credentials and by internally by   boto3 while looking for profiles.</p>"},{"location":"run_log_store/s3/#aws_access_key_name","title":"aws_access_key_name:","text":"<p>Defaults to  'AWS_ACCESS_KEY_ID'.   The environmental variable name that is to be used as aws access key, if you are using credentials.</p>"},{"location":"run_log_store/s3/#aws_secret_access_key_name","title":"aws_secret_access_key_name:","text":"<p>Defaults to 'AWS_SECRET_ACCESS_KEY'.   The environmental variable name that is used as AWS Secret access key, if you are using credentials.</p>"},{"location":"run_log_store/s3/#aws_session_key_name","title":"aws_session_key_name:","text":"<p>Defaults to 'AWS_SESSION_TOKEN'   The environmental variable name that is used for AWS session token, if you are using credentials.</p>"},{"location":"run_log_store/s3/#role_arn","title":"role_arn:","text":"<p>Defaults to ''.   The role to assume if you are using sessions.</p>"},{"location":"run_log_store/s3/#session_duration_in_seconds","title":"session_duration_in_seconds:","text":"<p>Defaults to 900   The duration of the AWS session.</p>"},{"location":"secrets/aws_secrets_manager/","title":"AWS Secrets manager","text":"<p>This package is an extension to magnus.</p>"},{"location":"secrets/aws_secrets_manager/#provides","title":"Provides","text":"<p>Provides functionality to use AWS secrets manager to provide secrets</p>"},{"location":"secrets/aws_secrets_manager/#installation_instructions","title":"Installation instructions","text":"<p><code>pip install magnus_extension_secrets_aws_secrets_manager</code></p>"},{"location":"secrets/aws_secrets_manager/#set_up_required_to_use_the_extension","title":"Set up required to use the extension","text":"<p>Access to AWS environment either via:</p> <ul> <li>AWS profile, generally stored in ~/.aws/credentials</li> <li>AWS credentials available as environment variables</li> </ul> <p>If you are using environmental variables for AWS credentials, please set:</p> <ul> <li>AWS_ACCESS_KEY_ID: AWS access key</li> <li>AWS_SECRET_ACCESS_KEY: AWS access secret</li> <li>AWS_SESSION_TOKEN: The session token, useful to assume other roles</li> </ul> <p>A AWS secrets store that you want to use to store the the secrets.</p>"},{"location":"secrets/aws_secrets_manager/#config_parameters","title":"Config parameters","text":"<p>The full configuration of the AWS secrets manager is:</p> <pre><code>secrets:\ntype: 'aws-secrets-manager'\nconfig:\nsecret_arn: The secret ARN to retrieve the secrets from.\nregion: # Region if we are using\naws_profile: #The profile to use or default\nuse_credentials: # Defaults to False\n</code></pre>"},{"location":"secrets/aws_secrets_manager/#secret_arn","title":"secret_arn:","text":"<p>The arn of the secret that you want to use. Internally, we use boto3 to access the secrets.</p> <p>The below parameters are inherited from AWS Configuration.</p>"},{"location":"secrets/aws_secrets_manager/#aws_profile","title":"aws_profile:","text":"<p>The profile to use for acquiring boto3 sessions.</p> <p>Defaults to None, which is used if its role based access or in case of credentials present as environmental variables.</p>"},{"location":"secrets/aws_secrets_manager/#region","title":"region:","text":"<p>The region to use for acquiring boto3 sessions.</p> <p>Defaults to eu-west-1.</p>"},{"location":"secrets/aws_secrets_manager/#aws_credentials_file","title":"aws_credentials_file:","text":"<p>The file containing the aws credentials.</p> <p>Defaults to <code>~/.aws/credentials</code>.</p>"},{"location":"secrets/aws_secrets_manager/#use_credentials","title":"use_credentials:","text":"<p>Set it to <code>True</code> to provide AWS credentials via environmental variables.</p> <p>Defaults to <code>False</code>.</p>"},{"location":"secrets/aws_secrets_manager/#role_arn","title":"role_arn:","text":"<p>The role to assume after getting the boto3 session.</p> <p>This is required if you are using <code>use_credentials</code>.</p>"}]}